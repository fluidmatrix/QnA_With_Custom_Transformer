# ğŸ§  Transformer-Based Text Summarization (TensorFlow)

An end-to-end implementation of an **abstractive text summarization model**
using a custom **Transformer (Encoderâ€“Decoder)** architecture built with
TensorFlow and Keras.

---

## âœ¨ Features

- Custom Transformer implementation (no high-level shortcuts)
- Encoderâ€“Decoder architecture with Multi-Head Attention
- Look-ahead and padding masks
- Teacher forcing during training
- Greedy decoding during inference
- SOS / EOS token-based sequence generation
- Fully reproducible training pipeline

---

## ğŸ“ Project Structure

transformer_model/
â”œâ”€â”€ main.py # Training & inference pipeline
â”œâ”€â”€ Transformer.py # Full Transformer model
â”œâ”€â”€ Encoder.py # Encoder stack
â”œâ”€â”€ Decoder.py # Decoder stack
â”œâ”€â”€ DecoderLayer.py # Masked attention decoder layer
â”œâ”€â”€ helper.py # Masks, preprocessing, utilities
â”œâ”€â”€ corpus/ # Training & test datasets
â”œâ”€â”€ requirements.txt # Dependencies (pip freeze)
â””â”€â”€ README.md


---

## ğŸ— Model Architecture

### Encoder
- Token embedding + positional encoding
- Stacked encoder layers
- Multi-head self-attention
- Feed-forward networks

### Decoder
- Masked self-attention (look-ahead)
- Encoderâ€“decoder attention (padding mask)
- Feed-forward network
- Final softmax over vocabulary

---

## âš™ Training Configuration

**Sequence Lengths**
- Encoder max length: `150`
- Decoder max length: `50`

**Hyperparameters**
- Embedding dimension: `128`
- Number of layers: `2`
- Attention heads: `2`
- Batch size: `64`
- Epochs: `20`

**Optimization**
- Optimizer: Adam
- Learning rate: Custom warmup schedule
- Loss: Masked Sparse Categorical Crossentropy

---

## ğŸ“‰ Loss Function

- Padding tokens are ignored
- Loss is computed only on valid tokens
- Normalized by number of non-padding tokens

---

## ğŸ” Inference & Summarization

### How Inference Works
1. Encode the input document
2. Initialize decoder with `[SOS]`
3. Predict tokens step-by-step
4. Stop at `[EOS]` or max length

### Example

**Input**
[SOS] amanda: i baked cookies... [EOS]


**Human Summary**
[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]


**Model Output**

Generated using greedy decoding
---

## â–¶ How to Run

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

